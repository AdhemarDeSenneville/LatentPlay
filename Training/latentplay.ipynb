{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8919108,"sourceType":"datasetVersion","datasetId":5364132}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"----\n\n# <center><b>Audio ResNet - LatentPlay</center>\n# <center><b><span style=\"color:red;\">Kick neural synthesis</b></span></center>\n----\n## Adhémar DE SENNEVILLE | adhemar.de_senneville@ens-paris-saclay.fr\n-----","metadata":{}},{"cell_type":"markdown","source":"# Initialization","metadata":{}},{"cell_type":"code","source":"RUN_NAME = 'RUN_11_Deeper_05_Compression' # WandB run name\nRUN_TEST = False # Do a fast run to check the pipeline is working\nRUN_1_BATCH = False # Do a fast run to check the pipeline is working\nMAX_EPOCH = 1000\n\nPROJECT = 'Latent_Play' # WandB project name, monitor audio outputs in real time\nHARDWARE = 'P100' # T4 or P100 or CPU\nDATA_SAVE = 'data' # Directory to save data","metadata":{"execution":{"iopub.status.busy":"2024-07-14T20:14:40.623397Z","iopub.execute_input":"2024-07-14T20:14:40.623799Z","iopub.status.idle":"2024-07-14T20:14:40.659535Z","shell.execute_reply.started":"2024-07-14T20:14:40.623766Z","shell.execute_reply":"2024-07-14T20:14:40.658352Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Imports\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport random\nimport time\nimport sys\nimport gc\nimport yaml\nimport pickle\nfrom math import floor\nfrom typing import Any, Dict, List, Optional, Sequence, Tuple, Union\n\n# Imports Torch\nimport torch\nimport torchaudio\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import random_split, DataLoader, TensorDataset, Dataset\nfrom torch.nn.utils.rnn import pad_sequence\nimport pytorch_lightning as pl\n\n# Imports Autre\nimport librosa\nimport matplotlib.pyplot as plt\nimport scipy.io.wavfile as wav\nimport scipy.signal as signal\nimport IPython.display as ipd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nsns.set(style=\"whitegrid\")\nos.makedirs('data', exist_ok=True)\nos.makedirs('fig', exist_ok=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-14T20:14:43.277388Z","iopub.execute_input":"2024-07-14T20:14:43.277789Z","iopub.status.idle":"2024-07-14T20:14:52.493913Z","shell.execute_reply.started":"2024-07-14T20:14:43.277756Z","shell.execute_reply":"2024-07-14T20:14:52.492799Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"### Kaggle setup","metadata":{}},{"cell_type":"code","source":"KAGGLE = os.getcwd() == \"/kaggle/working\"\n\n# Avoid pip install att each re-run\ntry:\n    print(\"Not first session\", FIRST_RUN)\nexcept:\n    print(\"First session\")\n    FIRST_RUN = True\n\nif KAGGLE and FIRST_RUN:\n    print(\"Notebook running on Kaggle\")\n    PATH = '/kaggle/input/kick-wav/kick_dataset'\n    \n    #!pip install -U 'wandb>=0.12.10' # NOT WORKING\n    #!pip uninstall -y wandb\n    #!pip install -U 'wandb==0.17.0' # Only version compatible...\n    !pip install auraloss\n    !pip install audio-encoders-pytorch\n    FIRST_RUN = False\nelse:\n    PATH = '../Dataset/kick_dataset' # Setup the path to you dataset\n    print(\"Notebook running on local\")","metadata":{"execution":{"iopub.status.busy":"2024-07-14T20:14:59.371847Z","iopub.execute_input":"2024-07-14T20:14:59.372440Z","iopub.status.idle":"2024-07-14T20:15:32.572208Z","shell.execute_reply.started":"2024-07-14T20:14:59.372404Z","shell.execute_reply":"2024-07-14T20:15:32.570764Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"First session\nNotebook running on Kaggle\nCollecting auraloss\n  Downloading auraloss-0.4.0-py3-none-any.whl.metadata (8.0 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from auraloss) (2.1.2+cpu)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from auraloss) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->auraloss) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->auraloss) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->auraloss) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->auraloss) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->auraloss) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->auraloss) (2024.3.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->auraloss) (2.1.3)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->auraloss) (1.3.0)\nDownloading auraloss-0.4.0-py3-none-any.whl (16 kB)\nInstalling collected packages: auraloss\nSuccessfully installed auraloss-0.4.0\nCollecting audio-encoders-pytorch\n  Downloading audio_encoders_pytorch-0.0.22-py3-none-any.whl.metadata (783 bytes)\nRequirement already satisfied: torch>=1.6 in /opt/conda/lib/python3.10/site-packages (from audio-encoders-pytorch) (2.1.2+cpu)\nRequirement already satisfied: torchaudio in /opt/conda/lib/python3.10/site-packages (from audio-encoders-pytorch) (2.1.2+cpu)\nCollecting data-science-types>=0.2 (from audio-encoders-pytorch)\n  Downloading data_science_types-0.2.23-py3-none-any.whl.metadata (5.4 kB)\nCollecting einops>=0.6 (from audio-encoders-pytorch)\n  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\nCollecting einops-exts>=0.0.3 (from audio-encoders-pytorch)\n  Downloading einops_exts-0.0.4-py3-none-any.whl.metadata (621 bytes)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->audio-encoders-pytorch) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->audio-encoders-pytorch) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->audio-encoders-pytorch) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->audio-encoders-pytorch) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->audio-encoders-pytorch) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->audio-encoders-pytorch) (2024.3.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6->audio-encoders-pytorch) (2.1.3)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6->audio-encoders-pytorch) (1.3.0)\nDownloading audio_encoders_pytorch-0.0.22-py3-none-any.whl (9.6 kB)\nDownloading data_science_types-0.2.23-py3-none-any.whl (42 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.7/42.7 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading einops-0.8.0-py3-none-any.whl (43 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading einops_exts-0.0.4-py3-none-any.whl (3.9 kB)\nInstalling collected packages: einops, data-science-types, einops-exts, audio-encoders-pytorch\nSuccessfully installed audio-encoders-pytorch-0.0.22 data-science-types-0.2.23 einops-0.8.0 einops-exts-0.0.4\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### WandB setup","metadata":{}},{"cell_type":"code","source":"# Put you WandB API Key here\nfrom kaggle_secrets import UserSecretsClient\nAPI_KEY = UserSecretsClient().get_secret(\"WandB_API_Key\")\nos.environ[\"WANDB_API_KEY\"] = API_KEY","metadata":{"execution":{"iopub.status.busy":"2024-07-14T20:16:38.889408Z","iopub.execute_input":"2024-07-14T20:16:38.889834Z","iopub.status.idle":"2024-07-14T20:16:39.718112Z","shell.execute_reply.started":"2024-07-14T20:16:38.889801Z","shell.execute_reply":"2024-07-14T20:16:39.716793Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### Seeding","metadata":{}},{"cell_type":"code","source":"from pytorch_lightning import seed_everything\nseed_everything(42, workers=True) # sets seeds for numpy, torch and python.random.","metadata":{"execution":{"iopub.status.busy":"2024-07-13T16:36:09.553983Z","iopub.execute_input":"2024-07-13T16:36:09.554362Z","iopub.status.idle":"2024-07-13T16:36:09.568051Z","shell.execute_reply.started":"2024-07-13T16:36:09.554322Z","shell.execute_reply":"2024-07-13T16:36:09.566907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"----\n# Utils\nDon't need to change that code","metadata":{}},{"cell_type":"code","source":"# Compute the max fft frequency (temporal 0 padding could be added for more precision)\ndef get_max_frequency(x, sr):\n    with torch.no_grad():\n        fft_result = torch.fft.fft(x)\n        power_spectrum = torch.abs(fft_result) ** 2\n        \n        # Get the frequency bin with the highest power\n        max_power_index = torch.argmax(power_spectrum)\n        max_frequency = max_power_index * sr / x.size(-1)\n        \n        return (max_frequency).item()\n\n# Compute the time pourcentage to get to the max amplitude\ndef get_attack(x):\n    attack_index = torch.argmax(x.abs())\n    return (attack_index/x.size(-1)).item()\n\n# Compute the time pourcentage to get below treshold\ndef get_release(x, threshold = 0.1):\n    \n    #print(x.shape)\n    x_end = x[0,:x.size(-1)//2] # min release at 50%\n    #print(x_end.shape)\n    indices_above_threshold = torch.where(x_end.abs() > threshold)[0]\n    #print(indices_above_threshold)\n    release_index = indices_above_threshold[-1]\n    return 0.5 - (release_index / x.size(-1)).item()\n\ndef clip(value, min_value, max_value):\n    return torch.clip(torch.tensor(value), min_value, max_value)","metadata":{"execution":{"iopub.status.busy":"2024-07-13T16:36:09.573000Z","iopub.execute_input":"2024-07-13T16:36:09.573405Z","iopub.status.idle":"2024-07-13T16:36:09.585096Z","shell.execute_reply.started":"2024-07-13T16:36:09.573369Z","shell.execute_reply":"2024-07-13T16:36:09.583812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data","metadata":{}},{"cell_type":"code","source":"class KickDataset(Dataset):\n    def __init__(self, path = \"\", sr = 44100, duration = 1, fade_out = 0.1):\n           \n        # Init variables\n        self.path = path\n        self.sr = sr\n        self.duration = duration\n        self.fade_out = fade_out\n        self.sample_length = int(sr * duration)\n        self.wav_files = self._create_index_table()\n\n    def _create_index_table(self):\n        wav_files = []\n        for root, _, files in os.walk(self.path):\n            for file in files:\n                if file.endswith('.wav'):\n                    relative_path = os.path.join(root, file)\n                    wav_files.append(relative_path)\n        return wav_files\n    \n    def load_all(self, limit=None):\n        # Load all dataset in local memory, util the limit\n        limit = len(self) if limit is None else min(limit, len(self))\n        self.data = torch.zeros((limit, 1, self.sample_length))\n        self.features = torch.zeros((limit, 3))\n\n        for i in range(limit):\n            x = self.load_wav(self.wav_files[i])\n            self.data[i] = x\n            \n            # Normalize and add features\n            self.features[i][0] = clip(get_max_frequency(x, self.sr),0,150)/150\n            self.features[i][1] = clip(get_attack(x),0,0.5)/0.5\n            self.features[i][2] = clip(get_release(x),0,0.5)/0.5\n\n    def __len__(self):\n        return len(self.wav_files)\n\n    def __getitem__(self, idx):\n        return self.data[idx], self.features[idx]\n    \n    def load_wav(self, relative_path):\n        # Load audio file\n        waveform, sample_rate = torchaudio.load(relative_path, normalize=True)\n\n        # Ensure mono by averaging channels if necessary\n        if waveform.size(0) > 1:\n            waveform = waveform.mean(dim=0, keepdim=True)\n\n        # Resample if sample rate is different from cfg\n        if sample_rate != self.sr:\n            transform = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=self.sr)\n            waveform = transform(waveform)\n\n        # Normalize the waveform\n        waveform = waveform / waveform.abs().max()\n\n        # Pad or trim to the desired sample length\n        if waveform.size(1) < self.sample_length:\n            pad_size = self.sample_length - waveform.size(1)\n            waveform = torch.nn.functional.pad(waveform, (0, pad_size))\n        elif waveform.size(1) > self.sample_length:\n            waveform = waveform[:, :self.sample_length]\n        \n         # Apply linear fade-out to the last 10% of the audio\n        fade_out_length = int(self.sample_length * 0.1)\n        fade_out = torch.linspace(1, 0, fade_out_length)\n        waveform[:, -fade_out_length:] *= fade_out\n\n        return waveform\n\n    def show_sample(self, idx):\n        waveform = self.load_wav(self.wav_files[idx])\n        print(\"Name  :\",self.wav_files[idx])\n        print(\"Shape :\",waveform.shape)\n        plt.figure(figsize=(10, 4))\n        plt.plot(waveform.t().numpy())\n        plt.title(f\"Waveform of sample at index {idx}\")\n        plt.xlabel(\"Time\")\n        plt.ylabel(\"Amplitude\")\n        plt.show()\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-13T16:36:09.586667Z","iopub.execute_input":"2024-07-13T16:36:09.587027Z","iopub.status.idle":"2024-07-13T16:36:09.605158Z","shell.execute_reply.started":"2024-07-13T16:36:09.586998Z","shell.execute_reply":"2024-07-13T16:36:09.603966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loss","metadata":{}},{"cell_type":"code","source":"import auraloss\nimport torch.nn as nn\n\n# I improve audio quality (from simple MSE used before)\n# By using this Multiresolution Time Frequency loss\n# It reduces the high frequency flickering I noticed using MSE Loss (time domain loss)\n\nclass TimeFrequencyLoss(nn.Module):\n    def __init__(self, alpha, tau, gain, sr, duration):\n        super().__init__()\n        \n        # Sample size ~6000\n        self.frequ_loss = auraloss.freq.MultiResolutionSTFTLoss(\n            fft_sizes=[32, 128, 512, 2048], #[32, 128, 512, 2048, 8192, 32768]\n            hop_sizes=[16, 64, 256, 1024], #[16, 64, 256, 1024, 4096, 16384]\n            win_lengths=[32, 128, 512, 2048], #[32, 128, 512, 2048, 8192, 32768]\n            w_sc=0.0,\n            w_phs=0.0,\n            w_lin_mag=1.0,\n            w_log_mag=1.0,\n        )\n        self.time_loss = nn.MSELoss()\n        self.alpha = alpha\n        \n        length = int(sr * duration)\n        t = torch.linspace(0,1,length)\n        self.enveloppe = 1 + gain * torch.exp(-t/tau)\n        \n\n    def forward(self, y_hat, y):\n        enveloppe = self.enveloppe.to(y_hat.device)\n        \n        y_hat_mod = y_hat * enveloppe\n        y_mod = y * enveloppe\n        \n        \n        # Calculate frequency domain loss\n        f_loss = self.frequ_loss(y_hat, y)\n        \n        # Calculate time domain loss\n        t_loss = self.time_loss(y_hat, y)\n        \n        # Combine the losses\n        total_loss = f_loss + self.alpha * t_loss\n        return total_loss","metadata":{"execution":{"iopub.status.busy":"2024-07-13T16:36:09.606360Z","iopub.execute_input":"2024-07-13T16:36:09.606703Z","iopub.status.idle":"2024-07-13T16:36:09.628040Z","shell.execute_reply.started":"2024-07-13T16:36:09.606668Z","shell.execute_reply":"2024-07-13T16:36:09.626818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"code","source":"from audio_encoders_pytorch import AutoEncoder1d #from model import AutoEncoder1d\n\nclass LitAutoEncoder(pl.LightningModule):\n    def __init__(self, model_cfg, training_cfg, data_cfg):\n        super(LitAutoEncoder, self).__init__()\n        self.save_hyperparameters() # for wandb\n        \n        # High level features of the AutoEncoder\n        embedded_length = int(data_cfg['duration'] * data_cfg['sr'])\n        for factor in model_cfg['factors']:\n            embedded_length = int((embedded_length-1)/factor)+1\n\n        input_channels = model_cfg['channels']\n        for multiplier in model_cfg['multipliers']:\n            input_channels = input_channels // multiplier\n        \n        # Dim values\n        self.length = int(data_cfg['duration'] * data_cfg['sr'])\n        self.compression_rate = model_cfg['compression_rate']\n        self.conv_out_channels = model_cfg['channels']\n        self.conv_out_length = embedded_length\n        self.conv_out_dim = self.conv_out_channels * self.conv_out_length\n        self.latent_dim = int(self.length * self.compression_rate)\n\n        # Model initialization\n        self.model = AutoEncoder1d(\n            in_channels=model_cfg['in_channels'],\n            channels=model_cfg['channels'],\n            multipliers=model_cfg['multipliers'],\n            factors=model_cfg['factors'],\n            num_blocks=model_cfg['num_blocks']\n        )\n        # Dense layers\n        self.encode_linear = nn.Linear(self.conv_out_dim, self.latent_dim)\n        self.decode_linear = nn.Linear(self.latent_dim, self.conv_out_dim)\n        self.features_linear = nn.Linear(self.latent_dim, 3)\n        \n        # Model infos\n        print(f'{\"Global Compression Rate\":>30} : {100 * self.compression_rate:.2f} % (~{int(0.5+1/self.compression_rate)})')\n        print(f'{\"Convolution Compression Rate\":>30} : {100 * self.conv_out_dim / self.length:.2f} % (~{int(0.5+self.length/self.conv_out_dim)})')\n        print(f'{\"Dense Compression Rate\":>30} : {100 * self.latent_dim / self.conv_out_dim:.2f} % (~{int(0.5+self.conv_out_dim/self.latent_dim)})')\n        print(f'{\"Input Shape\":>30} : ({input_channels},{self.length})')\n        print(f'{\"Conv Latent Shape\":>30} : ({self.conv_out_channels},{self.conv_out_length}) -> {self.conv_out_dim}')\n        print(f'{\"Latent Shape\":>30} : ({self.latent_dim})')\n        print(f'{\"Parameter Number\":>30} : ({self.count_parameters()})') \n        print(f'{\"Encoder + Decoder are\":>30} : Resnet-{int(np.sum(model_cfg[\"num_blocks\"])):_}')\n        \n        # Training init\n        self.lr = training_cfg['lr']\n        self.sr = data_cfg['sr']\n        self.audio_loss = TimeFrequencyLoss(**training_cfg['audio_loss_params'],\n                                         sr = training_cfg['sr'],\n                                         duration = training_cfg['duration'])\n        self.beta = training_cfg['features_loss_params']['beta']\n        self.features_loss = nn.MSELoss()\n\n        # Placeholder for the first batch, first audio samples\n        self.first_audio_sample = None\n        \n    def forward(self, x):\n        z = self.forward_encode(x)\n        f = self.forward_features(z)\n        x = self.forward_decode(z)\n        return x, z, f\n    \n    def forward_features(self, z):\n        f = self.features_linear(z)\n        return f\n    \n    def forward_encode(self, x):\n        z = self.model.encode(x)\n        z = z.flatten(1)\n        z = self.encode_linear(z)\n        return z\n    \n    def forward_decode(self,z):\n        z = self.decode_linear(z)\n        z = z.view(z.shape[0],self.conv_out_channels,self.conv_out_length)\n        x = self.model.decode(z)\n        return x[..., :self.length]\n    \n    def count_parameters(self):\n        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n    \n    def training_step(self, batch, batch_idx):\n        x, f = batch\n        \n        # Log the original audio\n        if self.first_audio_sample is None: \n            self.first_audio_sample = x[(0,),...]\n        \n        # Compute loss\n        x_hat, z, f_hat = self.forward(x)\n        audio_loss = self.audio_loss(x_hat, x)\n        features_loss = self.beta * self.features_loss(f_hat, f)\n        total_loss = audio_loss+features_loss\n        \n        # Log\n        self.log_dict({'train_loss': total_loss, \n                       'audio_loss': audio_loss,\n                       'features_loss':features_loss,\n                      }) #, on_step=True, on_epoch=True\n        return total_loss\n    \n    def predict_step(self, x, batch_idx):\n        x_hat = self.forward(x)\n        return x_hat\n\n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters(), lr=self.lr)\n    \n    def on_train_epoch_end(self):        \n        #print('here')\n        \n        if (self.first_audio_sample is not None) and (self.current_epoch % 10 == 1) and hasattr(self.logger.experiment, 'log'):\n            \n            # Get the first audio sample\n            #print(self.first_audio_sample.device)\n            original_audio = self.first_audio_sample[0].cpu().numpy()\n            reconstructed_audio = self.forward(self.first_audio_sample)[0][0].cpu().detach().numpy()\n            \n            # Ensure the audio data is in the correct range and format\n            if original_audio.dtype != 'float32':\n                original_audio = original_audio.astype('float32')\n            if reconstructed_audio.dtype != 'float32':\n                reconstructed_audio = reconstructed_audio.astype('float32')\n\n            # Normalize audio to be in the range -1.0 to 1.0\n            # original_audio /= np.max(np.abs(original_audio), axis=-1, keepdims=True)\n            # reconstructed_audio /= np.max(np.abs(reconstructed_audio), axis=-1, keepdims=True)\n            # print(original_audio.shape)\n            # print(reconstructed_audio.shape)\n            \n            # Log Audios\n            if self.current_epoch==1:\n                self.logger.experiment.log({\n                    \"original_audio\": wandb.Audio(original_audio[0], sample_rate=self.sr, caption=\"Original Audio\"),\n                    \"epoch\": self.current_epoch\n                })\n            self.logger.experiment.log({\n                \"reconstructed_audio\": wandb.Audio(reconstructed_audio[0], sample_rate=self.sr, caption=\"Reconstructed Audio\"),\n                \"epoch\": self.current_epoch\n            })\n            \n            ori_path = 'fig/original_spectrogram.png'\n            rec_path = 'fig/reconstructed_spectrogram.png'\n            # Compute spectrograms to decibel (dB) units\n            original_spectrogram = librosa.stft(original_audio[0])\n            reconstructed_spectrogram = librosa.stft(reconstructed_audio[0])\n            original_spectrogram_db = librosa.amplitude_to_db(np.abs(original_spectrogram), ref=np.max)\n            reconstructed_spectrogram_db = librosa.amplitude_to_db(np.abs(reconstructed_spectrogram), ref=np.max)\n\n            # Log Spectrograms\n            if self.current_epoch == 1:\n                plt.figure(figsize=(5, 5))\n                librosa.display.specshow(original_spectrogram_db, sr=self.sr, x_axis='time', y_axis='log')\n                plt.colorbar(format='%+2.0f dB')\n                plt.title('Original Spectrogram')\n                plt.tight_layout()\n                plt.savefig(ori_path)\n                plt.close()\n\n                self.logger.experiment.log({\n                    \"original_spectrogram\": wandb.Image(ori_path),\n                    \"epoch\": self.current_epoch\n                })\n            \n            plt.figure(figsize=(5, 5))\n            librosa.display.specshow(reconstructed_spectrogram_db, sr=self.sr, x_axis='time', y_axis='log')\n            plt.colorbar(format='%+2.0f dB')\n            plt.title('Reconstructed Spectrogram')\n            plt.tight_layout()\n            plt.savefig(rec_path)\n            plt.close()\n\n            self.logger.experiment.log({\n                \"reconstructed_spectrogram\": wandb.Image(rec_path),\n                \"epoch\": self.current_epoch\n            })\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-13T16:36:09.630155Z","iopub.execute_input":"2024-07-13T16:36:09.630590Z","iopub.status.idle":"2024-07-13T16:36:09.670896Z","shell.execute_reply.started":"2024-07-13T16:36:09.630551Z","shell.execute_reply":"2024-07-13T16:36:09.669656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Testing model inference","metadata":{}},{"cell_type":"code","source":"if False:\n    '''model_cfg = {\n        'in_channels': 1,\n        'channels': 64,\n        'multipliers': [1, 2, 2, 2, 2, 1, 1],\n        'factors':     [4, 4, 4, 4, 4, 2],\n        'num_blocks':  [5, 5, 5, 5, 5, 5],\n        'in_length': 6615,\n        'compression_rate': 0.01,\n    }\n\n    training_cfg = {\n        # Learning\n        'epoch': 400,\n        'epoch_min': 50,\n        'patience': 50,\n        'lr': 1e-4,\n\n        # Loss\n        'audio_loss_params':{\n            'alpha': 100,\n            'tau': 0.1,\n            'gain': 5,\n        },\n        'features_loss_params':{\n            'beta':50,\n        },\n\n        # Data\n        'sr': 22050,\n        'duration': 0.3,\n\n        # Other\n        'batch_size': 32,\n        'num_workers': 3,\n        'best_model_path': None,\n        'hardware': HARDWARE,\n        'machine': 'Kaggle' if KAGGLE else 'PC'\n    }\n    \n    data_cfg = {\n        'path': PATH,\n        'sr': 22050,\n        'duration': 0.3, # seconds\n        'fade_out': 0.1 # 10% fade out\n    }\n\n    # Initialize the model\n    model = LitAutoEncoder(model_cfg, training_cfg, data_cfg)\n\n    # Generate random input\n    input_tensor_1 = torch.randn(32, 1, 6615)\n    input_tensor_2 = torch.randn(32, 3)\n\n    # Quick inference encoder\n    laten = model.model.encoder(input_tensor_1)\n    print(f'{laten.shape = }')\n    \n    # Quick inference auto-encoder\n    output = model(input_tensor_1)#, input_tensor_2\n    print(f'{output[0].shape = }')'''","metadata":{"execution":{"iopub.status.busy":"2024-07-13T16:36:09.675340Z","iopub.execute_input":"2024-07-13T16:36:09.675753Z","iopub.status.idle":"2024-07-13T16:36:16.413527Z","shell.execute_reply.started":"2024-07-13T16:36:09.675724Z","shell.execute_reply":"2024-07-13T16:36:16.412392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"----\n# Training Configuration","metadata":{}},{"cell_type":"code","source":"training_cfg = {\n    # Learning\n    'epoch': MAX_EPOCH,\n    'epoch_min': 100,\n    'patience': 300,\n    'lr': 3e-5,\n    \n    # Loss\n    'audio_loss_params':{\n        'alpha': 100,\n        'tau': 0.1, # See README\n        'gain': 5,  # See README\n    },\n    'features_loss_params':{\n        'beta':50,\n    },\n    \n    # Data\n    'sr': 22050, # per seconds\n    'duration': 0.3, # in seconds\n    \n    # Other\n    'batch_size': 32,\n    'num_workers': 3,\n    'best_model_path': None,\n    'hardware': HARDWARE,\n    'machine': 'Kaggle' if KAGGLE else 'PC'\n}\n","metadata":{"execution":{"iopub.status.busy":"2024-07-13T16:36:16.414809Z","iopub.execute_input":"2024-07-13T16:36:16.415557Z","iopub.status.idle":"2024-07-13T16:36:16.421866Z","shell.execute_reply.started":"2024-07-13T16:36:16.415524Z","shell.execute_reply":"2024-07-13T16:36:16.420722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Config","metadata":{}},{"cell_type":"code","source":"data_cfg = {\n    'path': PATH, # Path to dataset\n    'sr': training_cfg['sr'],\n    'duration': training_cfg['duration'], \n    'fade_out': 0.1 # 10% fade out\n}\n\n# Create dataset instance\ndataset = KickDataset(**data_cfg)","metadata":{"execution":{"iopub.status.busy":"2024-07-13T16:36:16.423503Z","iopub.execute_input":"2024-07-13T16:36:16.423887Z","iopub.status.idle":"2024-07-13T16:36:16.837202Z","shell.execute_reply.started":"2024-07-13T16:36:16.423850Z","shell.execute_reply":"2024-07-13T16:36:16.836056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.load_all()\nmemory_usage_mb = sys.getsizeof(dataset.data.untyped_storage()) / (1024 ** 2)\nprint(f'Dataset Memory usage: {memory_usage_mb:.2f} MB')\nprint(f'Dataset Shape       : {dataset.data.shape}')\ntraining_cfg['time_length'] = dataset.data.shape[-1]","metadata":{"execution":{"iopub.status.busy":"2024-07-13T16:36:16.838460Z","iopub.execute_input":"2024-07-13T16:36:16.838795Z","iopub.status.idle":"2024-07-13T16:36:53.299409Z","shell.execute_reply.started":"2024-07-13T16:36:16.838766Z","shell.execute_reply":"2024-07-13T16:36:53.298185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Warning, No valdation, No test... the objectiv is to overfit \ndataloader = DataLoader(dataset, \n                        batch_size=training_cfg['batch_size'], \n                        shuffle=True, \n                        num_workers=training_cfg['num_workers'])","metadata":{"execution":{"iopub.status.busy":"2024-07-13T16:36:53.300889Z","iopub.execute_input":"2024-07-13T16:36:53.301258Z","iopub.status.idle":"2024-07-13T16:36:53.307794Z","shell.execute_reply.started":"2024-07-13T16:36:53.301228Z","shell.execute_reply":"2024-07-13T16:36:53.306444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Config","metadata":{}},{"cell_type":"code","source":"# Configuration dictionaries\nmodel_cfg = {\n        'in_channels': 1,\n        'channels': 64,\n        'multipliers': [1, 2, 2, 2, 2, 1, 1],\n        'factors':     [4, 4, 4, 4, 4, 2],\n        'num_blocks':  [6, 6, 6, 6, 6, 6],\n        'compression_rate': 0.005, # 0.5% Compression\n    }\n# Initialize model\nautoencoder = LitAutoEncoder(model_cfg, training_cfg, data_cfg)","metadata":{"execution":{"iopub.status.busy":"2024-07-13T16:36:53.309335Z","iopub.execute_input":"2024-07-13T16:36:53.309691Z","iopub.status.idle":"2024-07-13T16:36:53.418839Z","shell.execute_reply.started":"2024-07-13T16:36:53.309661Z","shell.execute_reply":"2024-07-13T16:36:53.417717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training Configs","metadata":{}},{"cell_type":"code","source":"if KAGGLE:\n    # Kaggle herdware\n    if HARDWARE == 'T4':\n        num_gpus = 2\n        available_devices = [0,1]\n        print('On Kaggle Double T4')\n    elif HARDWARE == 'P100':\n        num_gpus = 1\n        available_devices = [0]\n        print('On Kaggle P100')\n    else:\n        num_gpus = 0\n        available_devices = ['CPU']\n        print('On Kaggle CPU')\nelse:\n    num_gpus = 0 # No gpu on my computer :(\n    available_devices = ['CPU']\n    print('On laptop CPU')\n\naccelerator = 'gpu' if num_gpus > 0 else 'cpu'\ndevices = num_gpus if num_gpus > 0 else 1","metadata":{"execution":{"iopub.status.busy":"2024-07-13T16:36:53.420369Z","iopub.execute_input":"2024-07-13T16:36:53.420790Z","iopub.status.idle":"2024-07-13T16:36:53.429169Z","shell.execute_reply.started":"2024-07-13T16:36:53.420752Z","shell.execute_reply":"2024-07-13T16:36:53.427879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pytorch_lightning import Trainer\nfrom pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n\nearly_stop_callback = EarlyStopping(\n    monitor='train_loss',  \n    patience=training_cfg['patience'],          \n    verbose=True,        \n    mode='min'           \n)\n\ncheckpoint_callback = ModelCheckpoint(\n    monitor='train_loss',  # Change this to 'val_loss' if you have validation set\n    dirpath=f'{DATA_SAVE}/',\n    filename='best-checkpoint',\n    save_top_k=1,\n    mode='min'\n)\n\ntrainer_cfg = {\n    \n    # Hardware\n    'accelerator': accelerator,\n    'devices': devices,\n    'num_nodes': 1,\n    'precision': 32, # Float32\n    'deterministic': False, # Increase training speed for our fixed tensor size dataset\n    'benchmark': True, # Increase training speed for our fixed tensor size dataset\n    \n    # Epochs\n    'min_epochs': training_cfg['epoch_min'],\n    'max_epochs': training_cfg['epoch'],\n    'max_time': '00:12:00:00',\n    'accumulate_grad_batches': 1,\n    'callbacks': [early_stop_callback, checkpoint_callback],\n    \n    # Logging / Debug\n    'logger': None,  # Defined later\n    'profiler': None,   # Defined later\n    'fast_dev_run': RUN_TEST,\n    'limit_train_batches': 1 if RUN_1_BATCH else None,\n    'enable_checkpointing': True,\n    'barebones': False,\n}","metadata":{"execution":{"iopub.status.busy":"2024-07-13T16:36:53.430929Z","iopub.execute_input":"2024-07-13T16:36:53.431278Z","iopub.status.idle":"2024-07-13T16:36:53.444534Z","shell.execute_reply.started":"2024-07-13T16:36:53.431249Z","shell.execute_reply":"2024-07-13T16:36:53.443488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Logger Config","metadata":{}},{"cell_type":"code","source":"import wandb\nfrom pytorch_lightning.loggers import WandbLogger\nif False: # Bebug WAndB\n    \n    #os.environ[\"WANDB_HTTP_TIMEOUT\"] = \"300\"  # Useless\n    #os.environ[\"WANDB_MODE\"] = \"offline\"      # Not working\n    import wandb\n    wandb.login()\n    wandb.init(project=PROJECT, config=all_config)\n    wandb.finish()\n    \n    from lightning_utilities.core.imports import RequirementCache\n    _WANDB_AVAILABLE = RequirementCache(\"wandb>=0.12.10\")\n    _WANDB_AVAILABLE\n    \n    !ls /opt/conda/lib/python3.10/site-packages/wandb-0.17.0.dist-info\n    #!sudo /opt/conda/lib/python3.10/site-packages/wandb-0.17.0.dist-info/METADATA","metadata":{"execution":{"iopub.status.busy":"2024-07-13T16:36:53.446015Z","iopub.execute_input":"2024-07-13T16:36:53.446344Z","iopub.status.idle":"2024-07-13T16:36:54.541347Z","shell.execute_reply.started":"2024-07-13T16:36:53.446316Z","shell.execute_reply":"2024-07-13T16:36:54.540208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_config = {\n    'TRAINING': training_cfg,\n    'MODEL': model_cfg,\n    'DATA': data_cfg,\n    'TRAINER': trainer_cfg,\n}\n\nprofiler = 'simple' # Track training performance\nlogger = None if (RUN_TEST or RUN_1_BATCH) else WandbLogger(project=PROJECT, name=RUN_NAME, config=all_config)\n\ntrainer_cfg['profiler'] = profiler\ntrainer_cfg['logger'] = logger","metadata":{"execution":{"iopub.status.busy":"2024-07-13T16:36:54.542757Z","iopub.execute_input":"2024-07-13T16:36:54.543148Z","iopub.status.idle":"2024-07-13T16:36:54.549335Z","shell.execute_reply.started":"2024-07-13T16:36:54.543109Z","shell.execute_reply":"2024-07-13T16:36:54.548259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"----\n# Run","metadata":{}},{"cell_type":"code","source":"# Initialize model\nautoencoder = LitAutoEncoder(model_cfg, training_cfg, data_cfg)\n\n# Initialize trainer\ntrainer = Trainer(**trainer_cfg)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Here we go :\ntrainer.fit(autoencoder, dataloader)\nwandb.finish()","metadata":{"execution":{"iopub.status.busy":"2024-07-13T16:36:54.550899Z","iopub.execute_input":"2024-07-13T16:36:54.551228Z","iopub.status.idle":"2024-07-13T16:39:07.002480Z","shell.execute_reply.started":"2024-07-13T16:36:54.551201Z","shell.execute_reply":"2024-07-13T16:39:07.000930Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wandb.finish()","metadata":{"execution":{"iopub.status.busy":"2024-07-13T16:39:07.005008Z","iopub.execute_input":"2024-07-13T16:39:07.005522Z","iopub.status.idle":"2024-07-13T16:39:07.012101Z","shell.execute_reply.started":"2024-07-13T16:39:07.005471Z","shell.execute_reply":"2024-07-13T16:39:07.010665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if False:    # If you want to pause execution after training\n    print(\"Paused execution. Run the following line to continue.\")\n    while not input():\n        time.sleep(1)","metadata":{"execution":{"iopub.status.busy":"2024-07-13T16:39:07.013757Z","iopub.execute_input":"2024-07-13T16:39:07.014158Z","iopub.status.idle":"2024-07-13T16:39:07.028406Z","shell.execute_reply.started":"2024-07-13T16:39:07.014126Z","shell.execute_reply":"2024-07-13T16:39:07.027207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"----\n# Play with latent","metadata":{}},{"cell_type":"code","source":"# All parameters used for the generation of the vst\nlatent_play_parameters = {'sr': data_cfg['sr'],\n                         'audio_length': 6615,\n                         }","metadata":{"execution":{"iopub.status.busy":"2024-07-13T16:39:07.031616Z","iopub.execute_input":"2024-07-13T16:39:07.032286Z","iopub.status.idle":"2024-07-13T16:39:07.048130Z","shell.execute_reply.started":"2024-07-13T16:39:07.032245Z","shell.execute_reply":"2024-07-13T16:39:07.046324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    print(checkpoint_callback.best_model_path)\n    autoencoder = LitAutoEncoder.load_from_checkpoint('data/best-checkpoint.ckpt')\nexcept:\n    print('Impossible to load checkpoint')","metadata":{"execution":{"iopub.status.busy":"2024-07-13T16:39:07.050450Z","iopub.execute_input":"2024-07-13T16:39:07.051007Z","iopub.status.idle":"2024-07-13T16:39:07.946187Z","shell.execute_reply.started":"2024-07-13T16:39:07.050966Z","shell.execute_reply":"2024-07-13T16:39:07.944910Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load latent dataset\nZ = []\nfor i, batch in enumerate(dataset):\n    with torch.no_grad():\n        if i % 10 == 0:\n            print(round(100 * i / len(dataset), 2), '%', end='\\r')\n            \n        device = autoencoder.device\n        x, _ = batch\n        x_in = x.unsqueeze(0).to(device)\n        encoded_output = autoencoder.forward_encode(x_in).squeeze(0).cpu().detach().numpy()\n        Z.append(encoded_output)\n\n# Concatenate all encoded outputs\nZ = np.array(Z)\nprint(f\"Encoded Data Shape: {Z.shape}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-13T16:39:07.948249Z","iopub.execute_input":"2024-07-13T16:39:07.948716Z","iopub.status.idle":"2024-07-13T16:40:55.392550Z","shell.execute_reply.started":"2024-07-13T16:39:07.948673Z","shell.execute_reply":"2024-07-13T16:40:55.391324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## PCA","metadata":{}},{"cell_type":"code","source":"from sklearn.decomposition import PCA\n\n# Latent PCA\npca = PCA(n_components=2)\nprincipal_components = pca.fit_transform(Z)\nz_pca_1, z_pca_2 = pca.components_\n\nprint(f'{principal_components.shape = }')\nprint(f'{z_pca_1.shape = }')\n\n# The dataset is composed of different sample pack\n# Each sample pack can be a class\ny_classes = [path.split('kick_dataset/')[1].split('/')[0] for path in dataset.wav_files]","metadata":{"execution":{"iopub.status.busy":"2024-07-13T16:40:55.408350Z","iopub.execute_input":"2024-07-13T16:40:55.408865Z","iopub.status.idle":"2024-07-13T16:40:55.824810Z","shell.execute_reply.started":"2024-07-13T16:40:55.408801Z","shell.execute_reply":"2024-07-13T16:40:55.823459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save first and second components\nlatent_play_parameters[\"z_pca_1\"] = z_pca_1\nlatent_play_parameters[\"z_pca_2\"] = z_pca_2\n\nlatent_play_parameters[\"z_pca_1_scale\"] = (np.min(principal_components[:,0]),np.max(principal_components[:,0]))\nlatent_play_parameters[\"z_pca_2_scale\"] = (np.min(principal_components[:,1]),np.max(principal_components[:,1]))","metadata":{"execution":{"iopub.status.busy":"2024-07-13T16:40:55.852010Z","iopub.execute_input":"2024-07-13T16:40:55.853046Z","iopub.status.idle":"2024-07-13T16:40:55.866116Z","shell.execute_reply.started":"2024-07-13T16:40:55.853002Z","shell.execute_reply":"2024-07-13T16:40:55.864866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Clustering","metadata":{}},{"cell_type":"code","source":"# Nice cluster plot\nimport matplotlib.colors as mcolors\ncolors = list(mcolors.CSS4_COLORS.values())\n\n# Create a scatter plot\nplt.figure(figsize=(16, 14))\nunique_classes = list(set(y_classes))\n\nfor i, class_name in enumerate(unique_classes):\n    class_indices = [j for j, x in enumerate(y_classes) if x == class_name]\n    s = [10] * len(class_indices)\n    \n    # Plot points for each class\n    plt.scatter(principal_components[class_indices, 0], \n                principal_components[class_indices, 1], \n                s, color=colors[i], label=class_name, alpha=1)\n\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.title('2D Scatter Plot of Principal Components with Centroids')\nplt.grid(False)\nplt.legend()\n\nplt.tight_layout()\nplt.savefig('fig/pca_2D_space.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-13T16:40:55.869168Z","iopub.execute_input":"2024-07-13T16:40:55.870486Z","iopub.status.idle":"2024-07-13T16:40:58.284191Z","shell.execute_reply.started":"2024-07-13T16:40:55.870434Z","shell.execute_reply":"2024-07-13T16:40:58.283055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Nice map of packs in latent space\n# Create a scatter plot\nplt.figure(figsize=(16, 14))\nunique_classes = list(set(y_classes))\ncentroids = []\n\nfor i, class_name in enumerate(unique_classes):\n    class_indices = [j for j, x in enumerate(y_classes) if x == class_name]\n    s = [30] * len(class_indices)\n    \n    # Plot points for each class\n    plt.scatter(principal_components[class_indices, 0], \n                principal_components[class_indices, 1], \n                s, color=colors[i], label=class_name, alpha=1)\n    \n    # Compute and plot centroids\n    centroid = np.mean(principal_components[class_indices], axis=0)\n    centroids.append(centroid)\n    plt.scatter(centroid[0], centroid[1], s=100, color=colors[i], edgecolors='black', marker='X')\n    plt.text(centroid[0], centroid[1], class_name, fontsize=12, ha='center', va='center', \n             color='black', backgroundcolor=colors[i])\n\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.xlim((np.min(np.array(centroids)[:,0])*1.2,np.max(np.array(centroids)[:,0])*1.2))\nplt.ylim((np.min(np.array(centroids)[:,1])*1.2,np.max(np.array(centroids)[:,1])*1.2))\nplt.title('2D Scatter Plot of Principal Components with Centroids')\nplt.grid(False)\n\nplt.tight_layout()\nplt.savefig('fig/pca_cluster.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-13T16:40:58.285863Z","iopub.execute_input":"2024-07-13T16:40:58.286257Z","iopub.status.idle":"2024-07-13T16:40:59.854275Z","shell.execute_reply.started":"2024-07-13T16:40:58.286224Z","shell.execute_reply":"2024-07-13T16:40:59.853120Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Linking *High level features* and *Latent Space*","metadata":{}},{"cell_type":"markdown","source":"### Frequency feature","metadata":{}},{"cell_type":"code","source":"# Plotting the frequency number of predictions in a bar plot\nY_freq = dataset.features[:,0]\nunique_freqs, counts = np.unique(Y_freq, return_counts=True)\n\nplt.figure(figsize=(15, 6))  # Make the plot wider\nbars = plt.bar(unique_freqs, counts, color='black', edgecolor='black', width=0.02)\nplt.xlabel('Frequency (Hz)')\nplt.ylabel('Number of Predictions')\nplt.title('Frequency Distribution of Predictions')\n\n# Add grid lines\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.xlim(0, 1)\n\nplt.tight_layout()\nplt.savefig('fig/feature_freq.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-13T16:40:59.855928Z","iopub.execute_input":"2024-07-13T16:40:59.856349Z","iopub.status.idle":"2024-07-13T16:41:00.627848Z","shell.execute_reply.started":"2024-07-13T16:40:59.856313Z","shell.execute_reply":"2024-07-13T16:41:00.626731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Attack feature","metadata":{}},{"cell_type":"code","source":"# Plotting the attack number of predictions in a bar plot\nY_attack = dataset.features[:,1]\nunique_freqs, counts = np.unique(Y_attack, return_counts=True)\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 6))  # Create subplots\n\n# Main plot\nax1.bar(unique_freqs, counts, color='black', edgecolor='black', width=0.001)\nax1.set_xlabel('Attack (sample)')\nax1.set_ylabel('Number of Predictions')\nax1.set_title('Attack Distribution of Predictions')\nax1.grid(axis='y', linestyle='--', alpha=0.7)\nax1.set_xlim(0, 1)\n\n# Zoomed-in plot\nax2.bar(unique_freqs, counts, color='black', edgecolor='black', width=0.0002)\nax2.set_xlabel('Attack (sample)')\nax2.set_ylabel('Number of Predictions')\nax2.set_title('Zoomed-in Attack Distribution')\nax2.grid(axis='y', linestyle='--', alpha=0.7)\nax2.set_xlim(0, 0.1)\n\nplt.tight_layout()\nplt.savefig('fig/feature_attack.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-13T16:41:00.629221Z","iopub.execute_input":"2024-07-13T16:41:00.629572Z","iopub.status.idle":"2024-07-13T16:41:05.950723Z","shell.execute_reply.started":"2024-07-13T16:41:00.629544Z","shell.execute_reply":"2024-07-13T16:41:05.949634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Release feature","metadata":{}},{"cell_type":"code","source":"# Plotting the attack number of predictions in a bar plot\nY_release = dataset.features[:,2]\nunique_freqs, counts = np.unique(Y_release, return_counts=True)\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 6))  # Create subplots\n\n# Main plot\nax1.bar(unique_freqs, counts, color='black', edgecolor='black', width=0.001)\nax1.set_xlabel('Release (sample)')\nax1.set_ylabel('Number of Predictions')\nax1.set_title('Frequency Distribution of Predictions')\nax1.grid(axis='y', linestyle='--', alpha=0.7)\nax1.set_xlim(0, 0.2)\n\n# Zoomed-in plot\nax2.bar(unique_freqs, counts, color='black', edgecolor='black', width=0.0003)\nax2.set_xlabel('Release (sample)')\nax2.set_ylabel('Number of Predictions')\nax2.set_title('Zoomed-in Frequency Distribution')\nax2.grid(axis='y', linestyle='--', alpha=0.7)\nax2.set_xlim(0, 0.02)\nax2.set_ylim(0, 20)\n\nplt.tight_layout()\nplt.savefig('fig/feature_release.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-13T16:41:05.952230Z","iopub.execute_input":"2024-07-13T16:41:05.952674Z","iopub.status.idle":"2024-07-13T16:41:08.680403Z","shell.execute_reply.started":"2024-07-13T16:41:05.952634Z","shell.execute_reply":"2024-07-13T16:41:08.679240Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Post processing based on grafs","metadata":{}},{"cell_type":"code","source":"# To array\nY_freq_ = np.array(Y_freq)\nY_attack_ = np.array(Y_attack)\nY_release_ = np.array(Y_release)\n\n# Avoide outliers to over influence mse loss fit \n#Y_freq_ = np.clip(np.array(Y_freq_), 0, 150)\n#Y_attack_ = np.clip(np.array(Y_attack_), 0, 0.5)\n#Y_release_ = np.clip(np.array(Y_release_), 0, 0.5)\n\n# Normalizing\n#Y_freq_ = Y_freq_ / np.max(Y_freq_)\n#Y_attack_ = Y_attack_ / np.max(Y_attack_)\n#Y_release_ = Y_release_ / np.max(Y_release_)","metadata":{"execution":{"iopub.status.busy":"2024-07-13T16:41:08.681773Z","iopub.execute_input":"2024-07-13T16:41:08.682144Z","iopub.status.idle":"2024-07-13T16:41:08.688231Z","shell.execute_reply.started":"2024-07-13T16:41:08.682114Z","shell.execute_reply":"2024-07-13T16:41:08.687175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Linear regrassion","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n\n# Convert tensors to numpy arrays\nY_freq_ = Y_freq_.reshape(-1, 1)\nY_attack_ = Y_attack_.reshape(-1, 1)\nY_release_ = Y_release_.reshape(-1, 1)\n\n# Perform linear regression for each parameter\nregressor_freq = LinearRegression().fit(Z, Y_freq_)\nregressor_attack = LinearRegression().fit(Z, Y_attack_)\nregressor_release = LinearRegression().fit(Z, Y_release_)\n\n# Make predictions\nY_freq_pred = regressor_freq.predict(Z)\nY_attack_pred = regressor_attack.predict(Z)\nY_release_pred = regressor_release.predict(Z)\n\n# Get regression vectors\nreg_vector_freq = regressor_freq.coef_\nreg_vector_attack = regressor_attack.coef_\nreg_vector_release = regressor_release.coef_","metadata":{"execution":{"iopub.status.busy":"2024-07-13T16:41:08.689775Z","iopub.execute_input":"2024-07-13T16:41:08.690780Z","iopub.status.idle":"2024-07-13T16:41:08.814428Z","shell.execute_reply.started":"2024-07-13T16:41:08.690739Z","shell.execute_reply":"2024-07-13T16:41:08.812989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate MSE for each parameter\nmse_freq = mean_squared_error(Y_freq_, Y_freq_pred)\nmse_attack = mean_squared_error(Y_attack_, Y_attack_pred)\nmse_release = mean_squared_error(Y_release_, Y_release_pred)\n\n# Calculate MAPE for each parameter\nmape_freq = mean_absolute_percentage_error(Y_freq_, Y_freq_pred)\nmape_attack = mean_absolute_percentage_error(Y_attack_, Y_attack_pred)\nmape_release = mean_absolute_percentage_error(Y_release_, Y_release_pred)\n\n# Print the results\nprint(f\"Frequency - MSE: {mse_freq}, MAPE: {mape_freq}\")\nprint(f\"Attack - MSE: {mse_attack}, MAPE: {mape_attack}\")\nprint(f\"Release - MSE: {mse_release}, MAPE: {mape_release}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-13T16:41:08.817761Z","iopub.execute_input":"2024-07-13T16:41:08.819632Z","iopub.status.idle":"2024-07-13T16:41:08.841321Z","shell.execute_reply.started":"2024-07-13T16:41:08.819567Z","shell.execute_reply":"2024-07-13T16:41:08.840078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"latent_play_parameters[\"theta_freq\"] = reg_vector_freq[0]\nlatent_play_parameters[\"theta_attack\"] = reg_vector_attack[0]\nlatent_play_parameters[\"theta_release\"] = reg_vector_release[0]\nprint(reg_vector_freq[0].shape, reg_vector_attack[0].shape, reg_vector_release[0].shape)","metadata":{"execution":{"iopub.status.busy":"2024-07-13T16:41:08.843553Z","iopub.execute_input":"2024-07-13T16:41:08.845104Z","iopub.status.idle":"2024-07-13T16:41:08.856206Z","shell.execute_reply.started":"2024-07-13T16:41:08.845061Z","shell.execute_reply":"2024-07-13T16:41:08.855030Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Save data for the Plugin","metadata":{}},{"cell_type":"code","source":"#!cat data/config.yaml","metadata":{"execution":{"iopub.status.busy":"2024-07-13T16:41:08.859183Z","iopub.execute_input":"2024-07-13T16:41:08.860660Z","iopub.status.idle":"2024-07-13T16:41:08.869742Z","shell.execute_reply.started":"2024-07-13T16:41:08.860148Z","shell.execute_reply":"2024-07-13T16:41:08.868581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the dictionary of arrays to a file using pickle\nwith open(f'{DATA_SAVE}/latent_play_parameters.pkl', 'wb') as file:\n    pickle.dump(latent_play_parameters, file)\n    \nvst_config = {\n    'TRAINING': training_cfg,\n    'MODEL': model_cfg,\n    'DATA': data_cfg,\n}\n\nwith open(f'{DATA_SAVE}/config.yaml', 'w') as file:\n    yaml.dump(vst_config, file)\n\n# Check the created files\nimport os\nos.system('ls data')","metadata":{"execution":{"iopub.status.busy":"2024-07-13T16:41:08.871470Z","iopub.execute_input":"2024-07-13T16:41:08.872252Z","iopub.status.idle":"2024-07-13T16:41:08.922985Z","shell.execute_reply.started":"2024-07-13T16:41:08.872208Z","shell.execute_reply":"2024-07-13T16:41:08.921880Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Control Latent Space","metadata":{}},{"cell_type":"code","source":"z_pca_1 = latent_play_parameters[\"z_pca_1\"]\nz_pca_2 = latent_play_parameters[\"z_pca_2\"]\ntheta_freq = latent_play_parameters[\"theta_freq\"]\ntheta_attack = latent_play_parameters[\"theta_attack\"]\ntheta_release = latent_play_parameters[\"theta_release\"]\n\nA = np.stack([z_pca_1, z_pca_2, theta_freq, theta_attack, theta_release])\nC = A.T @ np.linalg.inv(A @ A.T)\n\ndef control_latent(z,\n                   latent_pca1,\n                   latent_pca2,\n                   target_freq,\n                   target_attack,\n                   target_release,\n                   ):\n    \n    global A, C2\n    \n    # Make z satisfy the targets with minimum change according linear regression\n    b = np.array([latent_pca1, latent_pca2, target_freq, target_attack, target_release]) # !!\n    z_prim = z - C @ (A @ z - b)\n\n    return z_prim","metadata":{"execution":{"iopub.status.busy":"2024-07-13T16:41:08.924696Z","iopub.execute_input":"2024-07-13T16:41:08.925138Z","iopub.status.idle":"2024-07-13T16:41:08.935985Z","shell.execute_reply.started":"2024-07-13T16:41:08.925100Z","shell.execute_reply":"2024-07-13T16:41:08.934773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Et voilà !","metadata":{}}]}