{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# <center><b>Audio ResNet - VAE</center>\n",
    "# <center><b><span style=\"color:red;\">Kick neural synthesis</b></span></center>\n",
    "----\n",
    "## AdhÃ©mar DE SENNEVILLE | adhemar.de_senneville@ens-paris-saclay.fr\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import floor\n",
    "import numpy as np\n",
    "from typing import Any, Dict, List, Optional, Sequence, Tuple, Union\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "PATH = '../Dataset/kick_dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torchaudio\n",
    "import torch\n",
    "\n",
    "class KickDataset(Dataset):\n",
    "    def __init__(self, path = \"\", sr = 44100, duration = 1):\n",
    "                 \n",
    "        self.path = path\n",
    "        self.sr = sr\n",
    "        self.duration = duration\n",
    "        self.sample_length = int(sr * duration)\n",
    "        self.wav_files = self._create_index_table()\n",
    "\n",
    "    def _create_index_table(self):\n",
    "        wav_files = []\n",
    "        for root, _, files in os.walk(self.path):\n",
    "            for file in files:\n",
    "                if file.endswith('.wav'):\n",
    "                    relative_path = os.path.join(root, file)\n",
    "                    wav_files.append(relative_path)\n",
    "        return wav_files\n",
    "    \n",
    "    def load_all(self, limit=None):\n",
    "        limit = len(self) if limit is None else min(limit, len(self))\n",
    "        self.data = torch.zeros((limit, 1, self.sample_length))\n",
    "\n",
    "        for i in range(limit):\n",
    "            self.data[i] = self.load_wav(self.wav_files[i])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.wav_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "    \n",
    "    def load_wav(self, relative_path):\n",
    "        # Load audio file\n",
    "        waveform, sample_rate = torchaudio.load(relative_path, normalize=True)\n",
    "\n",
    "        # Ensure mono by averaging channels if necessary\n",
    "        if waveform.size(0) > 1:\n",
    "            waveform = waveform.mean(dim=0, keepdim=True)\n",
    "\n",
    "        # Resample if sample rate is different from cfg\n",
    "        if sample_rate != self.sr:\n",
    "            transform = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=self.sr)\n",
    "            waveform = transform(waveform)\n",
    "\n",
    "        # Normalize the waveform\n",
    "        waveform = waveform / waveform.abs().max()\n",
    "\n",
    "        # Pad or trim to the desired sample length\n",
    "        if waveform.size(1) < self.sample_length:\n",
    "            pad_size = self.sample_length - waveform.size(1)\n",
    "            waveform = torch.nn.functional.pad(waveform, (0, pad_size))\n",
    "        elif waveform.size(1) > self.sample_length:\n",
    "            waveform = waveform[:, :self.sample_length]\n",
    "\n",
    "        return waveform\n",
    "\n",
    "    def show_sample(self, idx):\n",
    "        waveform = self.load_wav(self.wav_files[idx])\n",
    "        print(\"Name  :\",self.wav_files[idx])\n",
    "        print(\"Shape :\",waveform.shape)\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.plot(waveform.t().numpy())\n",
    "        plt.title(f\"Waveform of sample at index {idx}\")\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(\"Amplitude\")\n",
    "        plt.show()\n",
    "\n",
    "data_cfg = {\n",
    "    'path': PATH,\n",
    "    'sr': 22050,\n",
    "    'duration': 0.3, # seconds\n",
    "}\n",
    "\n",
    "# Create dataset instance\n",
    "dataset = KickDataset(**data_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from model import AutoEncoder1d\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "class LitAutoEncoder(pl.LightningModule):\n",
    "    def __init__(self, model_cfg, training_cfg):\n",
    "        super(LitAutoEncoder, self).__init__()\n",
    "\n",
    "        # Model initialization\n",
    "        self.model = AutoEncoder1d(\n",
    "            in_channels=model_cfg['in_channels'],\n",
    "            channels=model_cfg['channels'],\n",
    "            multipliers=model_cfg['multipliers'],\n",
    "            factors=model_cfg['factors'],\n",
    "            num_blocks=model_cfg['num_blocks']\n",
    "        )\n",
    "\n",
    "        # Training \n",
    "        self.lr = training_cfg['lr']\n",
    "        self.loss = training_cfg['loss']\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, x, batch_idx):\n",
    "        \n",
    "        x_hat = self.forward(x)\n",
    "        loss = self.loss(x_hat, x)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration dictionaries\n",
    "model_cfg = {\n",
    "    'in_channels': 2,\n",
    "    'channels': 32,\n",
    "    'multipliers': [1, 1, 2, 2],\n",
    "    'factors': [4, 4, 4],\n",
    "    'num_blocks': [2, 2, 2]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_cfg = {\n",
    "    'batch_size': 32,\n",
    "    'max_epochs': 10,\n",
    "    'lr': 1e-3\n",
    "}\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "autoencoder = LitAutoEncoder(model_cfg)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=training_cfg['batch_size'], shuffle=True)\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = pl.Trainer(max_epochs=training_cfg['max_epochs'], gpus=1 if torch.cuda.is_available() else 0)\n",
    "\n",
    "# Train model\n",
    "trainer.fit(autoencoder, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Lattente space analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = np.array(torch.cat([autoencoder.model.encoder(x.unsqueeze(0)) for x in dataset]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link High level features and latent space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Features \n",
    "Here we compute some easy to compute high level features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_frequency(x, sr):\n",
    "    with torch.no_grad():\n",
    "        fft_result = torch.fft.fft(x)\n",
    "        power_spectrum = torch.abs(fft_result) ** 2\n",
    "        \n",
    "        # Get the frequency bin with the highest power\n",
    "        max_power_index = torch.argmax(power_spectrum)\n",
    "        max_frequency = max_power_index * sr / x.size(-1)\n",
    "        \n",
    "        return (max_frequency).item()\n",
    "\n",
    "def get_attack(x):\n",
    "    attack_index = torch.argmax(x.abs())\n",
    "    # Time 100, because values will be low\n",
    "    return (100*attack_index/x.size(-1)).item()\n",
    "\n",
    "def get_release(x):\n",
    "    threshold = 0.1\n",
    "    indices_above_threshold = torch.where(x.abs() > threshold)[0]\n",
    "    if len(indices_above_threshold) > 0:\n",
    "        release_index = indices_above_threshold[-1]\n",
    "        return (release_index / x.size(-1)).item()\n",
    "    else:\n",
    "        return 0  # If no samples are above the threshold (souldn't be possible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_freq = np.array([get_max_frequency(x,dataset.sr) for x in dataset])\n",
    "Y_attack = np.array([get_attack(x) for x in dataset])\n",
    "Y_release = np.array([get_release(x) for x in dataset])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regrassion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Convert tensors to numpy arrays\n",
    "Y_freq = Y_freq.reshape(-1, 1)\n",
    "Y_attack = Y_attack.reshape(-1, 1)\n",
    "Y_release = Y_release.reshape(-1, 1)\n",
    "\n",
    "# Perform linear regression for each parameter\n",
    "regressor_freq = LinearRegression().fit(Z, Y_freq)\n",
    "regressor_attack = LinearRegression().fit(Z, Y_attack)\n",
    "regressor_release = LinearRegression().fit(Z, Y_release)\n",
    "\n",
    "# Get regression vectors\n",
    "reg_vector_freq = regressor_freq.coef_\n",
    "reg_vector_attack = regressor_attack.coef_\n",
    "reg_vector_release = regressor_release.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Control from latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.stack([reg_vector_freq, reg_vector_attack, reg_vector_release])\n",
    "\n",
    "\n",
    "def control_latent(z,\n",
    "                   target_freq,\n",
    "                   target_attack,\n",
    "                   target_release,\n",
    "                   latent_pca1,\n",
    "                   latent_pca2,\n",
    "                   ):\n",
    "    \n",
    "    global A, pca1, pca2\n",
    "    \n",
    "    # Change main varience in latente space\n",
    "    z = z + pca1 * latent_pca1 + pca2 * latent_pca2\n",
    "    \n",
    "    # Make z satisfy the targets with minimum change according linear regression\n",
    "    b = np.array([target_freq, target_attack, target_release]) # !!\n",
    "    z_prim = z - A.T @ np.linalg.inv(A @ A.T) @ (A @ z - b)\n",
    "\n",
    "    return z_prim\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MVA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
