
# Latent Play - VST

## -> [**Youtube Demo**](./code/Experiments_WaveShaper.ipynb) <-

[**Overview**](./code/Experiments_WaveShaper.ipynb)
| [**Tutorial**](#Tutorial)
| [**Presentation**](DDSP_Presentation.pptx)

## Presentation

Latent Play is a demo VST pluging for sample generation. It uses neural synthesise to allow infinit comtrol over your sample packs. This repository includes [Kaggle](https://www.kaggle.com/code/adhmardesenneville/latentplay/output?scriptVersionId=188128696) code using [Pytorch Lightning ](https://lightning.ai/docs/pytorch/stable/) and [WandB](https://wandb.ai/site) for the training pippline, and [Pyqt5](https://pypi.org/project/PyQt5/) for the GUI.


![](./fig/VST_view.png)

### How does it works
- 1) Train a model on on / several of your favorite sample packs. 
- 2) Load the sample of your choise
- Play with the PCA and Custom controls
- Select the sample of your choise


# Tutorial

### Dataset
```
Dataset
├── Pack 1
│   ├── Audio_1.wav
│   └── Audio_e.wav
├── Pack 2
│   └── SubPack 1
│       └── Audio_r.wav
│       └── Audio_X.wav
│   └── SubPack 2
│       └── Audio_F.wav
├── Pack 3
...
```
  
Having your sample separated in different pack allow clear and where each pack is sample in the latent space

### Configuration
In the code you can change, sample duration, auto encoder duration
model deep, training hardware, training time ect...


### Training
Launch the script and monitor the training via WandB API. You can have accest also to audio sample generated by your model during training to monitor audio quality.

![](./fig/training_loss.png)
![](./fig/training_audio.png)
![](./fig/training_spect.png)

### Run the VST
Put the training data output file from the run in the models folder
MODEL_PATH = ... 
Put your dataset in the dataset folder
DATASET_PATH = ...

run the application
```
python GUI/main.py
```


# Implementation details

### Latent Space Modification

this part of the project has been puporsly done without any look at the literatur nor popular technics to alow mor creativ thinking 
This could be the case that thos technics are either trivial or inexistant in the literatur

### Latent Space Control

Let $z$ represent the latent space. The objective is to find an application in this latent space z -> z' in a meaningful way. We aim to achieve specific modifications to control 5 parameters

1. $\lambda'_1 = \langle z', w_{(1)} \rangle$ : The projection of $ z $ in the Frirst Principal conponents

2. $\lambda'_2 = \langle z', w_{(2)} \rangle$ : The projection of $z$ in the Frirst Principal conponents

3. $f' = \langle z',\theta_{\text{f}} \rangle$ : Estimated Audio Frequency from Latent Space

4. $\alpha' = \langle z',\theta_{\alpha} \rangle$ : Estimated Audio Attack from Latent Space

5. $\beta' = \langle z',\theta_{\beta} \rangle$ : Estimated Audio Release from Latent Space



Those constrain then can  be conviniently expressed as a linear matrix equality $Az' = b$, where:
```math
A = 
\begin{bmatrix}
 & - - - & w_1 & - - - & \\
 & - - - & w_2 & - - - & \\
 & - - - & \theta_{\rho} & - - - & \\
 & - - - & \theta_{\alpha} & - - - & \\
 & - - - & \theta_{\beta} & - - - & \\
\end{bmatrix}, b' = \begin{bmatrix}
 & \lambda'_1& \\
 & \lambda'_2& \\
 & f' & \\
 & \alpha'& \\
 & \beta' & \\
\end{bmatrix}
```

### Now 
Now we also want that z' to be 'close' to z in the latent space, our ditance metric is l2 norm between $z$  and $z'$ : $\|z - z'\|^2$


#### 
our new point z' must solve
```math
\begin{aligned}
    & \underset{z'}{\text{minimize}}
    & & \|z - z'\|^2 \\
    & \text{subject to}
    & & Az' = b'
\end{aligned}
```


This is a classical convex optimisation problem.
Analiitical solution is :

The solution to that 
$$
z' = z + A^T (A A^T)^{-1} (A z - b) = z + A^T (A A^T)^{-1} (b - b') = 
$$
### Training Config
Here you can change the training config

### DATA

| Key       | Value                             |
|-----------|-----------------------------------|
| duration  | 0.3                               |
| fade_out  | 0.1                               |
| sr        | 22050                             |
| batch_size           | 32                     |

### MODEL

| Key               | Value                    |
|-------------------|--------------------------|
| channels          | 64                       |
| compression_rate  | 0.03                     |
| factors           | 4, 4, 4, 4, 4            |
| in_channels       | 1                        |
| multipliers       | 1, 2, 2, 2, 2, 1         |
| num_blocks        | 5, 5, 5, 5, 5            |

### TRAINING

| Key                  | Value                  |
|----------------------|------------------------|
| audio_loss_params    | alpha: 100, gain: 5, tau: 0.1 |
| epoch                | 800                    |
| epoch_min            | 50                     |
| patience             | 100                    |
| lr                   | 0.0001                 |
| features_loss_params | beta: 50               |
| num_workers          | 3                      |
| hardware             | P100                   |
| machine              | Kaggle                 |


### Model

Model is Auto encoder constitute of a ResNet of 1d convolution and a dense layer. 
The model is not a variational auto encoder, because of fiew resons.
It alows to load directly the samples from your sample pack in the vst plugging by picking the corresponding point in the latent space
It alows better reconstruction quality

### Loss

First experiment using mse temporal loss had high frequency ...
Using the time frequency multy rsolution loss, the results were much better
Also an other default of neural synthesis of the cick, the attack (0.05 first second) is really important to the final impression for us. This issue was improoved using a weighted loss that has hight ponderation at the begining. This was achieved using a multiplier anvlop parametrised as $e(t) = 1 + Ke^{-\frac{t}{\tau}}$

$$
L(x,x_{hat},f,f_{hat}) = AURA_{loss}(xe(t),x_{hat}e(t)) + \alpha \|xe(t) - x_{hat}e(t)\|^2 + \beta \|f - f_{hat}\|^2 
$$

| Loss Config |       |
|-------------|---------------|
| $\alpha$    | 100           |
| $\beta$     | 20            |
| K           | 5             |
| $\tau$      | 0.1           |



## Ethical Consurns

Unfortunatly the model has been trained on sample pack
neither the dataset and the output model can be givent. Especially given the fact that the model decder weight plus knowing latent space points is able to reproduce the entire private dataset whith high quality.

#

- Tonal and saturated, 
- subi / no sub
- Long tale - short transient
- Punch at the start
- top clip
- High ends
- Transient part - boby part - tail
- Sine sweep
- Dry (ne reverb) vs wet
- Round vs dirty
